# TorchClippedOptimizers

`torch-clip` a library to improve optimization methods by clipping off heavy-tailed gradient. This makes it possible to increase the accuracy and speed of convergence during the training of neural networks on a specific number of tasks.
<br>*Example of the distribution of gradient lengths with heavy tails:*
![This is an image](readme_images/heavy_tail.jpg)
------------
### our clipping methods

+ [Linear Stoch Norm Clipping](#LinearStochNormClip);  
+ [Quadratic Stoch Norm Clipping](#QuadraticStochNormClip);  
+ [Layer Wise Clipping](#LayerWiseClip);  
+ [Coordinate Wise Clipping](#CoordWiseClip);  
+ [Auto Clipping](#AutoClip);  
+ [Linear Stoch Auto Clipping](#LinearStochAutoClip);  
+ [Quadratic Stoch Auto Clipping](#QuadraticStochAutoClip).


###### <a name="LinearStochNormClip"></a>	Linear Stoch Norm Clipping
about this clipping methods  


###### <a name="QuadraticStochNormClip"></a>	Quadratic Stoch Norm Clipping
about this clipping methods  


###### <a name="LayerWiseClip"></a>	Layer Wise Clipping
about this clipping methods  


###### <a name="CoordWiseClip"></a>	Coordinate Wise Clipping
about this clipping methods  


###### <a name="AutoClip"></a>	Auto Clipping
about this clipping methods  


###### <a name="LinearStochAutoClip"></a>	Linear Stoch Auto Clipping
about this clipping methods  


###### <a name="QuadraticStochAutoClip"></a>	Quadratic Stoch Auto Clipping
about this clipping methods  
