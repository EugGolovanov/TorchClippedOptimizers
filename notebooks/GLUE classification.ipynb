{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTRIsUHOQPWE"
      },
      "source": [
        "## Installing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoSFeG-EsyDL"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install transformers datasets textaugment gensim==3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgoptATMLn85"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/EugGolovanov/TorchClippedOptimizers.git\n",
        "!mv /content/TorchClippedOptimizers/optimizers.py ./optimizers.py "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inxMo4V9QHh5"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJo9k0nPtQG9"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from torch.optim import Adam, AdamW\n",
        "from optimizers import clipped_SGD\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, MultiStepLR\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm import tqdm\n",
        "from datasets import concatenate_datasets, DatasetDict\n",
        "from optimizers_refactored import clipped_SGD\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from gensim import downloader\n",
        "from textaugment.word2vec import Word2vec\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjIzZTpGt8Sd"
      },
      "source": [
        "## Set seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LP9m306QXFT"
      },
      "outputs": [],
      "source": [
        "seed = 0xCAFEC0DE\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS70Kn_IuIaZ"
      },
      "source": [
        "## Dataset preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2218peFos2OG"
      },
      "outputs": [],
      "source": [
        "cola_dataset = load_dataset(\"glue\", \"cola\")\n",
        "sst_dataset = load_dataset(\"glue\", \"sst2\")\n",
        "rte_dataset = load_dataset(\"glue\", \"rte\")\n",
        "\n",
        "datasets_for_classification = {\"cola\": cola_dataset, \"sst2\": sst_dataset, \"rte\":rte_dataset}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpkFi4MS2d8K",
        "outputId": "b2197696-f15b-4b35-b0c4-7fd24daa5395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset cola\n",
            "Subdataset train: 1 - 6023, 0 - 2528\n",
            "Subdataset validation: 1 - 721, 0 - 322\n",
            "Subdataset test: -1 - 1063\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Dataset sst2\n",
            "Subdataset train: 0 - 29780, 1 - 37569\n",
            "Subdataset validation: 1 - 444, 0 - 428\n",
            "Subdataset test: -1 - 1821\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Dataset rte\n",
            "Subdataset train: 1 - 1241, 0 - 1249\n",
            "Subdataset validation: 1 - 131, 0 - 146\n",
            "Subdataset test: -1 - 3000\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for name, dataset_for_classification in datasets_for_classification.items():\n",
        "    print(f\"Dataset {name}\")\n",
        "    \n",
        "    for subdataset_type in dataset_for_classification.keys():\n",
        "        counts = Counter(dataset_for_classification[subdataset_type][\"label\"])\n",
        "        print(f\"Subdataset {subdataset_type}: {', '.join([f'{key} - {value}' for key, value in counts.items()])}\")\n",
        "    \n",
        "    print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SfalT0Yt_bF"
      },
      "source": [
        "## Load pretrain Word2Vec for text augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcFYvC0VfxhS"
      },
      "outputs": [],
      "source": [
        "word2vec_model = downloader.load('fasttext-wiki-news-subwords-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfPXuy5HdESt"
      },
      "outputs": [],
      "source": [
        "word2vec_augmenter = Word2vec(model=word2vec_model, p=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW-RX9HOP3wY"
      },
      "source": [
        "## Text augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eF_sfzUfmrN"
      },
      "outputs": [],
      "source": [
        "def augment_dataset(word2vec_augmenter):\n",
        "    def augment(x):\n",
        "        return {\"sentence\": word2vec_augmenter.augment(x[\"sentence\"]),\n",
        "                \"label\": x[\"label\"]}\n",
        "    \n",
        "    return augment\n",
        "\n",
        "concated_cola_dataset = DatasetDict()\n",
        "is_augmented = {\"train\": True, \"validation\":False}\n",
        "\n",
        "for dataset_type in [\"train\", \"validation\"]:\n",
        "    if is_augmented[dataset_type]:\n",
        "        dataset_for_concatenation = cola_dataset[dataset_type].filter(lambda x: x[\"label\"] == 0)\n",
        "        dataset_for_concatenation = dataset_for_concatenation.map(augment_dataset(word2vec_augmenter))\n",
        "        \n",
        "        concated_cola_dataset[dataset_type] = concatenate_datasets([cola_dataset[dataset_type], \n",
        "                                                                    dataset_for_concatenation])\n",
        "    else:\n",
        "        concated_cola_dataset[dataset_type] = cola_dataset[dataset_type]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBK8GQ1uNc5"
      },
      "source": [
        "## Functions of preparing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncwwYuG6vDsj"
      },
      "outputs": [],
      "source": [
        "def create_prepare_tokens_function(tokenizer, max_length=256):\n",
        "    def prepare_tokens(samples):\n",
        "        tokenized_sentence = tokenizer(samples[\"sentence\"],\n",
        "                                       padding=\"max_length\",\n",
        "                                       max_length=max_length)\n",
        "        return tokenized_sentence\n",
        "    \n",
        "    return prepare_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81WQ_luS65i-"
      },
      "outputs": [],
      "source": [
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, data_container: Dataset):\n",
        "        self.data = data_container\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row_data = self.data[idx]\n",
        "        \n",
        "        x = {key:torch.LongTensor(row_data[key]) for key in [\"input_ids\", \"attention_mask\"]}\n",
        "        y = row_data[\"label\"]\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5fu2krHuYrW"
      },
      "source": [
        "## Functions of training an epoch and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCsvFbcn_HQ0"
      },
      "outputs": [],
      "source": [
        "def train_fn(model, dataloader: DataLoader,\n",
        "             loss_fn: callable, metrics: Dict[str, callable], optimizer):\n",
        "    model.train()\n",
        "    \n",
        "    running_loss = 0\n",
        "    running_losses = []\n",
        "    metrics_values = {key: [] for key in metrics}\n",
        "    \n",
        "    for data, label in dataloader:\n",
        "      # convert data to gpu/cpu\n",
        "        data = {key: value.to(device) for key, value in data.items()}\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(**data).logits\n",
        "        preds = output.argmax(axis=-1)\n",
        "        # calculate loss\n",
        "        loss = loss_fn(output, label.to(device))\n",
        "        loss.backward()\n",
        "        \n",
        "        # nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
        "\n",
        "        optimizer.step()\n",
        "        # logging losses for plot\n",
        "        running_losses.append(loss.item())\n",
        "        # logging losses for dataframe\n",
        "        running_loss += loss.item()\n",
        "        # logging metrics\n",
        "        for metric_name in metrics:\n",
        "            if \"precision\" in metric_name or \"recall\" in metric_name:\n",
        "                metrics_values[metric_name].append(metrics[metric_name](label, preds.cpu(), zero_division=0))\n",
        "            else:\n",
        "                metrics_values[metric_name].append(metrics[metric_name](label, preds.cpu()))\n",
        "            \n",
        "    return running_loss / len(dataloader), metrics_values, running_losses\n",
        "\n",
        "def eval_fn(model, dataloader: DataLoader,\n",
        "             loss_fn: callable, metrics: Dict[str, callable]):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    metrics_values = {key: [] for key in metrics}\n",
        "    running_losses = []\n",
        "    for data, label in dataloader:\n",
        "        # convert data to device\n",
        "        x = {key: value.to(device) for key, value in data.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(**data).logits\n",
        "        preds = output.argmax(axis=-1)\n",
        "        # calculate loss\n",
        "        loss = loss_fn(output, label.to(device))\n",
        "        # logging loss\n",
        "        running_loss += loss.item()\n",
        "        running_losses.append(loss.item())\n",
        "        # logging metrics\n",
        "        for metric_name in metrics:\n",
        "            if \"precision\" in metric_name or \"recall\" in metric_name:\n",
        "                metrics_values[metric_name].append(metrics[metric_name](label, preds.cpu(), zero_division=0))\n",
        "            else:\n",
        "                metrics_values[metric_name].append(metrics[metric_name](label, preds.cpu()))\n",
        "        \n",
        "    return running_loss / len(dataloader), metrics_values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a pipeline from training and validating functions "
      ],
      "metadata": {
        "id": "wsNJtZQ9SY6A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ0aY91lO9TW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
        "\n",
        "def calculate_weights(labels):\n",
        "    # counting frequncies of classes\n",
        "    counts = Counter(labels)\n",
        "    \n",
        "    weights = [0 for _ in range(len(counts))]\n",
        "    \n",
        "    for key, value in counts.items():\n",
        "        # initialize the weights by the class frequency\n",
        "        weights[key] = 2 * len(labels) / value\n",
        "        \n",
        "    weights = torch.FloatTensor(weights)\n",
        "    \n",
        "    return weights\n",
        "\n",
        "\n",
        "def check_hypothesis(model_name, dataset, epochs, batch_size=32,\n",
        "                     weighted=False, metrics_for_logging=dict(),\n",
        "                     lr=1e-2, n_classes=2, optimizer=None, opt_params=None):\n",
        "    '''\n",
        "      input: model_name - > str: name of model for download from huggingface-hub\n",
        "      input: dataset - > torch.Dataset: data set\n",
        "      input: epochs - > int: num of epochs\n",
        "      input: batch_size - > int: batch size\n",
        "      input: weighted - > bool: flag of initialization weights\n",
        "      input: metrics_for_logging - > dict: dict of metrics for logging\n",
        "      input: lr - > float: learning rate\n",
        "      input: n_classes - > int: number of classes\n",
        "      input: optimizer - > torch.optim: custom optimizer\n",
        "      input: opt_params - > dict: params of optimizer \n",
        "      output: model - > torch.tensor: model weights\n",
        "      output: train_losses - > list: train losses logs by epoch\n",
        "      output: train_metrics - > dict: train metrics by epoch\n",
        "      output: eval_losses - > list: validation losses by epoch\n",
        "      output: eval_metrics - > dict: validation metrics by iter\n",
        "      output: long_train_losses - > list: \n",
        "    '''\n",
        "    # load pretrain model from huggingface-hub\n",
        "    model_config = AutoConfig.from_pretrained(model_name, num_labels=n_classes)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=model_config,\n",
        "                                                              ignore_mismatched_sizes=True)\n",
        "    model = model.to(device)\n",
        "    # prepare dataset\n",
        "    tokenized_dataset = dataset.map(create_prepare_tokens_function(tokenizer), batched=True)\n",
        "    train_dataset = ClassificationDataset(tokenized_dataset[\"train\"])\n",
        "    eval_dataset = ClassificationDataset(tokenized_dataset[\"validation\"])\n",
        "    # make weights conditioned on distribution of classes in dataset\n",
        "    if weighted:\n",
        "        weights = calculate_weights(dataset[\"train\"][\"label\"])\n",
        "    else:\n",
        "        weights = torch.FloatTensor([1 for _ in range(n_classes)])\n",
        "    # init loaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
        "    # make logging lists\n",
        "    train_losses, train_metrics, eval_losses, eval_metrics, long_train_losses, long_val_accs, long_train_accs = [], [], [], [], [], [], []\n",
        "    # prepare metric function\n",
        "    for key in metrics_for_logging.keys():\n",
        "        if \"f1\" in key:\n",
        "            metrics_for_logging[key] = lambda x, y: f1_score(x, y, average=\"micro\")\n",
        "    # set optimizer and scheduler\n",
        "    optimizer = optimizer(model.parameters(), **opt_params)\n",
        "    scheduler = MultiStepLR(optimizer, milestones=[3, 5, 7, 9], gamma=0.25)\n",
        "    \n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # train 1 epoch\n",
        "        train_loss, train_metric, long_train_loss = train_fn(model, train_dataloader,\n",
        "                                            CrossEntropyLoss(weights.to(device)), metrics_for_logging, optimizer)\n",
        "        # validate \n",
        "        eval_loss, eval_metric = eval_fn(model, eval_dataloader, CrossEntropyLoss(weights.to(device)), \n",
        "                                         metrics_for_logging)\n",
        "        scheduler.step()\n",
        "        # logging for plotting\n",
        "        long_train_losses.extend(long_train_loss)\n",
        "        long_train_accs.extend(train_metric['accuracy'])\n",
        "        long_eval_accs.extend(eval_metric['accuracy'])\n",
        "        # logging for dataframe\n",
        "        train_losses.append(np.mean(train_loss))\n",
        "        train_metrics.append({key: np.mean(value) for key, value in train_metric.items()})\n",
        "        eval_metrics.append({key: np.mean(value) for key, value in eval_metric.items()})\n",
        "        # eval_losses.append(eval_loss)\n",
        "        # eval_metrics.append(eval_metric)\n",
        "\n",
        "    return model, train_losses, train_metrics, eval_losses, eval_metrics, long_train_losses, long_train_accs, long_eval_accs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxRm906GPoyK"
      },
      "source": [
        "## *Optimizer*`s configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ye_rXiSPO9TW"
      },
      "outputs": [],
      "source": [
        "optimizers = [\n",
        "    {\n",
        "        'name': lambda lr: f\"Adam, {lr}, eps 1e-8, weight_decay 0.0005\",\n",
        "        'params': {'lr': 1e-4, 'eps': 1e-8, 'weight_decay':0.0005},\n",
        "        'opt': torch.optim.Adam\n",
        "    },\n",
        "    {\n",
        "        'name': lambda lr: f\"SGD, {lr}, 0.9\",\n",
        "        'params': {'lr':0.005, 'momentum':0.9},\n",
        "        'opt': torch.optim.SGD\n",
        "    },\n",
        "    {\n",
        "        'name': lambda lr: f\"clipped_SGD, {lr}, 0.9, norm, 3\",\n",
        "        'params': {'lr':3e-3, 'momentum':0.9, \n",
        "                    'clipping_type':\"norm\", 'clipping_level':3},\n",
        "        'opt': clipped_SGD\n",
        "    },\n",
        "    {\n",
        "        'name': lambda lr: f\"clipped_SGD, {lr}, 0.9, autoclip, 2, 0.75\",\n",
        "        'params': {'lr': 3e-3, 'momentum': 0.9, 'clipping_type': 'auto_clip',\n",
        "                    'p_autoclip': 0.75},\n",
        "        'opt': clipped_SGD\n",
        "    },\n",
        "    {\n",
        "        'name': lambda lr: f\"clipped_SGD, {lr}, 0.9, linear_stoch_clip_norm, 2, 0.85\",\n",
        "        'params': {'lr': 3e-3, 'momentum': 0.9,\n",
        "                      'clipping_type': \"linear_stoch_norm\", 'clipping_level': 2.0, 'beta': 0.85},\n",
        "        'opt': clipped_SGD\n",
        "    },\n",
        "    {\n",
        "        'name': lambda lr: f\"clipped_SGD, {lr}, 0.9, quadratic_stoch_norm, 2, 0.85\",\n",
        "        'params': {'lr': 3e-3, 'momentum': 0.9,\n",
        "                      'clipping_type': \"quadratic_stoch_norm\", 'clipping_level': 2.0, 'beta': 0.85},\n",
        "        'opt': clipped_SGD\n",
        "    },\n",
        "    {\n",
        "        'name': lambda lr: f\"clipped_SGD, {lr}, 0.9, linear_stoch_autoclip, 2, 0.85, 0.75\",\n",
        "        'params': {'lr': 3e-3, 'momentum': 0.9,\n",
        "                    'clipping_type': \"linear_stoch_autoclip\", 'clipping_level': 1.0, 'p_autoclip': 0.25, 'beta':0.85},\n",
        "        'opt': clipped_SGD\n",
        "    },\n",
        "    {\n",
        "        'name': lambda lr: f\"clipped_SGD, {lr}, 0.9, quadratic_stoch_autoclip, 2, 0.85, 0.75\",\n",
        "        'params': {'lr': 3e-3, 'momentum': 0.9,\n",
        "                    'clipping_type': \"quadratic_stoch_autoclip\", 'clipping_level': 1.0, 'p_autoclip': 0.25, 'beta':0.85},\n",
        "        'opt': clipped_SGD\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x-KaR4ktet7"
      },
      "source": [
        "## Check and log hypothesises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmwWk5mA8RY0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "logging_data = {x: [] for x in (\"Model name\", \"Dataset\", \"Loss\", \"Lr\",\n",
        "                               \"accuracy\", \"f1\", \"precision\", \"recall\", 'opt_name')}\n",
        "plot_logging_data = []\n",
        "for opt_data in optimizers:\n",
        "    for dataset_name, dataset in [(\"sst\", sst_dataset)]:\n",
        "        for model_name in ['bert-base-uncased']:\n",
        "            for lr in [2e-2, 3e-3, 5e-5, 5e-3, 1e-4]:\n",
        "                # prepare optimizer config for init \n",
        "                opt_name, opt_params, optimizer = opt_data['name'], opt_data['params'], opt_data['opt']\n",
        "                opt_params['lr'] = lr\n",
        "                opt_name = opt_name(lr)\n",
        "\n",
        "                model, train_losses, train_metrics, eval_losses, eval_metrics, long_train_losses, long_train_metric, long_eval_metric = check_hypothesis(model_name, dataset, 5, 32, True,\n",
        "                                                                                                                                                         {\"accuracy\": accuracy_score, \"f1\": f1_score,\n",
        "                                                                                                                                                          \"precision\": precision_score, \"recall\": recall_score},\n",
        "                                                                                                                                                          optimizer=optimizer, opt_params=opt_params)\n",
        "                # logs\n",
        "                logging_data[\"Model name\"].append(model_name)\n",
        "                logging_data[\"Dataset\"].append(dataset_name)\n",
        "                logging_data[\"Loss\"].append(min(eval_losses))\n",
        "                logging_data[\"Lr\"].append(lr)\n",
        "                logging_data['opt_name'].append(opt_name)\n",
        "                logging_data[\"accuracy\"].append(max(map(lambda x: x[\"accuracy\"], eval_metrics)))\n",
        "                plot_logging_data.append({\n",
        "                    'name': opt_name,\n",
        "                    'train_accuracy': long_train_metric,\n",
        "                    'val_accuracy': long_eval_metric,\n",
        "                    'train_loss': long_train_losses\n",
        "                })\n",
        "                f1_maximum_index = np.argmax(list(map(lambda x: x[\"f1\"], eval_metrics)))\n",
        "            \n",
        "                for key, value in eval_metrics[f1_maximum_index].items():\n",
        "                    if key != \"accuracy\":\n",
        "                        logging_data[key].append(value)\n",
        "                        \n",
        "                for key, value in logging_data.items():\n",
        "                    print(f\"{key}: {value[-1]}\", end=\", \")\n",
        "                print()\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot logs"
      ],
      "metadata": {
        "id": "LnJgdfF1X6qD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fsj6iVL1O9TX"
      },
      "outputs": [],
      "source": [
        "def make_plot(main_title, losses, train_acc, val_acc, names_optimizers,\n",
        "              metric_name=\"accuracy\"):\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 8))\n",
        "    fig.suptitle(main_title, fontsize=20)\n",
        "\n",
        "    ax1 = plt.subplot2grid((2, 5), (0, 0), rowspan=2, colspan=3)\n",
        "    ax2 = plt.subplot2grid((2, 5), (0, 3), colspan=2)\n",
        "    ax3 = plt.subplot2grid((2, 5), (1, 3), colspan=2)\n",
        "\n",
        "    fontdict={'fontsize': 14, 'fontweight': 'medium'}\n",
        "    ax1.set_title(f\"Train loss\", fontdict=fontdict)\n",
        "    ax2.set_title(f\"Train {metric_name}\", fontdict=fontdict)\n",
        "    ax3.set_title(f\"Valid {metric_name}\", fontdict=fontdict)\n",
        "\n",
        "    for i in range(len(losses)):\n",
        "        ax1.plot(losses[i], label=names_optimizers[i],  alpha=0.5)\n",
        "    ax1.legend()\n",
        "    ax1.grid()\n",
        "\n",
        "    for i in range(len(train_acc)):\n",
        "        ax2.plot(train_acc[i], label=names_optimizers[i],  alpha=0.5)\n",
        "    ax2.legend()\n",
        "    ax2.grid()\n",
        "\n",
        "    for i in range(len(val_acc)):\n",
        "        ax3.plot(val_acc[i], label=names_optimizers[i],alpha=0.5)\n",
        "    ax3.legend()\n",
        "    ax3.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGBVPZDyO9TY"
      },
      "outputs": [],
      "source": [
        "def item_from_arrofdict(arr_dct, item):\n",
        "    for dct in arr_dct:\n",
        "        yield dct[item]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGAFlEeUO9TY"
      },
      "outputs": [],
      "source": [
        "make_plot('BERT + CoLA&SST-2', item_from_arrofdict(plot_logging_data, 'train_loss'), item_from_arrofdict(plot_logging_data, 'train_accuracy'),\n",
        "           item_from_arrofdict(plot_logging_data, 'val_accuracy'), item_from_arrofdict(plot_logging_data, 'name'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export log as DataFrame"
      ],
      "metadata": {
        "id": "_psbQQNzb1dk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxofyCVvTIE8"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(logging_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFj_pd-fLdl4"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzjg_WRzLdl5"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"logs_classification.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GLUE_classification (1).ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}